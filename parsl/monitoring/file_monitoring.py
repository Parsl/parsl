"""
This module defines the file monitoring infrastructure.
"""
import datetime
import logging
import glob
from functools import wraps
import typeguard
import socket
import os
import time
from multiprocessing import pool, Event, Pool
import dill

from typing import List, Callable, Any, Dict, Union, Optional, Tuple, Pattern
from parsl.multiprocessing import ForkProcess

logger = logging.getLogger(__name__)


def proc_callback(res: Any) -> None:
    """Callback function for the results of running a function in the Pool.

       Writes result of the monitoring run to the log

       Parameters
       ----------
       res: Any
           Can really be anything that can be cast directly to a string.
    """
    # if there is no message
    if not res:
        return
    # type check the input and cast as appropriate
    if not isinstance(res, str):
        if isinstance(res, (bytes, bytearray)):
            try:
                res = res.decode()
            except Exception as ex:
                logger.error(f"Could not decode bytes like object: {str(ex)}")
                return
        else:
            try:
                res = str(res)
            except Exception as ex:
                logger.error(f"Could not turn data into string: {str(ex)}")
                return
    logging.info(res)


def run_dill_encoded(payload: str) -> Any:
    """Take the dill encoded payload, decode, and run it.

    Parameters
    ----------
    payload: str
        String generated by calling dill.dumps

    Returns
    -------
    Any
        The result of running the decoded function
    """
    fun, args = dill.loads(payload)
    return fun(*args)


def apply_async(monitor_pool: Any,  # cannot be Pool because of multiprocessing type weirdness.
                fun: Callable, args: Any) -> pool.AsyncResult:
    """Encode the given function and run it asynchronously.

    This is used to get around the pickl issue of not being able to encode functions that
    are not visible at the global scope.

    Parameters
    ----------
    monitor_pool: mp.Pool
        Pool object use to run the function
    fun: Callable
        The function to encode and run
    args: Any
        The function arguments

    Returns
    -------
    mp.Pool.AsyncResult
    """
    payload = dill.dumps((fun, args))
    return monitor_pool.apply_async(run_dill_encoded, (payload,), callback=proc_callback)


@typeguard.typechecked
def monitor(task_id: int,
            stop_event: Any,  # cannot be Event because of multiprocessing type weirdness.
            done_event: Any,  # cannot be Event because of multiprocessing type weirdness.
            patterns: List[Tuple[Callable, Union[str, Pattern]]],
            sleep_dur: float,
            work_dir: Optional[str] = None,
            max_proc: int = 2) -> None:
    """Function to monitor the file system for specific types of files and call a function when they are found.

    This function runs in a periodic loop unitl it is told to stop. The time between loops is goverened by `sleep_dur`
    seconds. Multiprocessing.Event objects are used to signal when this function should terminate. The general
    workflow is as follows::
                                        ------>callack (async)
                                        |
                     -----(found)-------+-------
                     |                         |
    Start----->scan for files---(none found)---+-->sleep --
                    ^                                     |
                    |                                     |
                    ---------------------------------------

    Any files that match any given pattern are tracked and only submitted to the callbacks once.

    Parameters
    ----------
    task_id: int
        The Parsl task id to be monitored.
    stop_event: multiprocessing.Event
        Signal for when to stop running the loop
    done_event: nultiprocessing.Event
        Signal used to indicate when this is actually completed, alowing for currently running callbacks to
        complete nicely.
    patterns: list
        List of tuples containing a callback and a regex or glob type statement for finding the files.
    sleep_dur: float
        The length of time to sleep between loops in seconds.
    work_dir: str
        The working directory for this process to run in. Default is ``None``, meaning the working directory is
        inherited from the calling process.
    max_proc: int, optional
        The maximum size of the multiprocessing Pool for the file processing. Default is 2, but the
        value is adjusted to be ``min(max_proc, len(patterns))`` so as to not make the pool larger than
        needed.
    """
    try:
        if work_dir is not None:
            os.chdir(work_dir)
        monitor_pool = Pool(min(max_proc, len(patterns)))

        logger.info(f"Monitor host {socket.gethostname()} started for task {task_id}")
        found = []
        keep_running = True
        running_procs = []  # keep track of all callback runs
        while keep_running:
            # see if the function has been told to stop
            keep_running = not stop_event.is_set()
            if not keep_running:
                logger.info(f"  {stop_event.is_set()} received {str(datetime.datetime.now())}")
                sleep_dur = 0
            # loop over all the patterns looking for new matches
            for i, (callback, pattern) in enumerate(patterns):
                logger.debug(f" {i} {pattern}")
                xfer = []
                current_time = time.time()
                # look for any matching files
                if isinstance(pattern, Pattern):
                    temp = [f for f in os.listdir(os.getcwd()) if pattern.search(f)]
                else:
                    temp = glob.glob(pattern)
                # weed out those that have been found before and any that are too new
                for t in temp:
                    if t in found:
                        continue
                    mtime = os.path.getmtime(t)
                    # make sure the file is done.
                    if mtime + sleep_dur < current_time:
                        xfer.append(t)
                if not xfer:
                    logger.debug(f"No files found for processing task {task_id}, pattern {i}.")
                    continue
                logger.debug(f"Found {len(xfer)} files for processing")
                # matches were found, sending them to callback
                running_procs.append(apply_async(monitor_pool, callback, (xfer,)))
                found += xfer
            if not keep_running:
                # the loop is terminating, wait for callbacks to finish
                monitor_pool.close()
                monitor_pool.join()
                monitor_pool.terminate()
            else:
                time.sleep(sleep_dur)
        logger.debug(f"HALT called for task {task_id}")
        # signal the function is complete
    except Exception as ex:
        logger.error(f"File monitor failed: {str(ex)}")
    finally:
        done_event.set()


@typeguard.typechecked
class FileMonitor:
    """The FileMonitor calss is an interface for defining any intermediate files that need to be processed mid-run.
    See :ref:`file-monitor-label` for a more detailed description of this system.

    Parameters
    ----------
    pattern: List, List[Tuple[Callable, Union[str, Pattern]]]
        A list of tuples containing the callback to use and a regex pattern (pre-compiled) or filetype. If the pattern
        is a filetype it can be with or without an asterisk and period (e.g. ``pdf``, ``.pdf``, and ``*.pdf`` all are
        valid and mean the same thing). The callback is called whenever any files matching the given pattern are found.
        The callback is run on the worker node does not have access to submit side processes.
    path: str, optional
        The base path where the files are expected to be, default is current working directory (``None``).
    working_dir: str, optional
        The working directory for the file monitoring, default (``None``) is the current working directory
        inherited when the monitor process is started.
    sleep_dur: float, optional
        The time to wait between scans of the file system to look for matching files. Default is 60 seconds.
    max_proc: int, optional
        The maximum size of the multiprocessing Pool for the file processing. Default is 2, but the
        value is adjusted to be ``min(max_proc, len(patterns))`` so as to not make the pool larger than
        needed, or overload the worker node.
    """
    def __init__(self,
                 pattern: List[Tuple[Callable, Union[str, Pattern]]],
                 path: Optional[str] = None,
                 working_dir: Optional[str] = None,
                 sleep_dur: float = 60.,
                 max_proc: int = 2):
        logger.info(f"File_monitor initialized: {path}, {working_dir}, {sleep_dur}")
        # generate the master pattern list
        self.patterns: List[Tuple[Callable, Union[str, Pattern]]] = []
        for (func, pat) in pattern:
            if isinstance(pat, str):
                if '*' not in pat:
                    if not pat.startswith('.'):
                        pat = '.' + pat
                    pat = '*' + pat
                if path is not None:
                    pat = os.path.join(path, pat)
            self.patterns.append((func, pat))
        self.sleep_dur = sleep_dur
        self.cwd = working_dir
        self.max_proc = max_proc

    def file_monitor(self,
                     f: Any,
                     task_id: int) -> Callable:
        """Function wrapper for launching the monitoring

        Parameters
        ----------
        f: Callable
            The function to wrap
        task_id: int
            The Parsl task id to be monitored.

        Returns
        -------
        Callable
            The given function wrapped by the monitoring code.

        """
        @wraps(f)
        def wrapped(*args: List[Any], **kwargs: Dict[str, Any]) -> Any:
            ev = Event()
            done = Event()
            pp = ForkProcess(target=monitor,
                             args=(task_id,
                                   ev,
                                   done,
                                   self.patterns,
                                   self.sleep_dur,
                                   self.cwd,
                                   self.max_proc))
            pp.start()
            try:
                return f(*args, **kwargs)
            finally:
                ev.set()
                pp.join(self.sleep_dur)
                if pp.exitcode is None:
                    pp.terminate()
                    pp.join()
        return wrapped
